{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import io, os\n",
    "import urllib2\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from keras.preprocessing import sequence\n",
    "from passage.preprocessing import Tokenizer\n",
    "\n",
    "red = '\\033[01;31m'\n",
    "native = '\\033[m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "\n",
    "First lets download some textfiles to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_and_save_file(url, filename):\n",
    "    response = urllib2.urlopen(url)\n",
    "    with io.open(filename, 'w', encoding='utf8') as fout:\n",
    "        fout.write(response.read().decode('utf-8'))\n",
    "\n",
    "input_lemmatized_url = 'https://raw.githubusercontent.com/alvations/stubboRNNess/master/cwi_inputs.lemmatized.txt'\n",
    "output_label_url = 'https://raw.githubusercontent.com/alvations/stubboRNNess/master/cwi_labels.txt'\n",
    "\n",
    "# let's download the file first.\n",
    "get_and_save_file(input_lemmatized_url, 'cwi_inputs.lemmatized.txt')\n",
    "get_and_save_file(output_label_url, 'cwi_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we take a look at what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('cwi_inputs.lemmatized.txt', 'r', encoding='utf8') as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        \n",
    "        print (red + 'Line:' + native, end='\\n')\n",
    "        print (line, end='\\n\\n')\n",
    "        print (red + 'Focus Word:' + native, end='\\n')\n",
    "        print (line.split(' <s> ')[0], end='\\n\\n')\n",
    "        print (red + 'Context:' + native, end='\\n')\n",
    "        print (line.split(' <s> ')[1], end='\\n\\n')\n",
    "        break\n",
    "        \n",
    "with io.open('cwi_labels.txt', 'r', encoding='utf8') as fin:\n",
    "    for line in fin:\n",
    "        print (red + 'Label:' + native, end='\\n')\n",
    "        line = line.strip()\n",
    "        print (line, end='\\n\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting strings to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('cwi_inputs.lemmatized.txt', 'r', encoding='utf8') as fin:\n",
    "    # There's repeating sentences in this dataset, thus the list(set()).\n",
    "    train_text = list(set([line.split(' <s> ')[1].lower().strip() for line in fin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "train_tokens = tokenizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will show you the mapping from tokens to their \"dictionary\" IDs\n",
    "tokenizer.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will show you the mapping from the \"dictionary\" IDs to the tokens.\n",
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at a sentence.\n",
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how you convert a SINGLE sentence to their IDs.\n",
    "tokenizer.transform(train_text[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how you convert a list of sentences to their IDs.\n",
    "tokenizer.transform(train_text[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting the vocabulary size\n",
    "\n",
    "We can limit the vocabulary size of our tokenizer by using the `max_features` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(max_features = 20)\n",
    "train_tokens = tokenizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.transform(train_text[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset\n",
    "\n",
    "Now that we know how to preprocess our corpus into IDs quickly, we'll look at a pre-processed dataset used for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "                                                      test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets take a look at the sentences.\n",
    "X_train[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ([len(_d) for _d in X_train[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what we want to do is remove pathologically long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try redoing pad_sequences()\n",
    "max_features = 20000\n",
    "maxlen = 170  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "                                                      test_split=0.2)\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ([len(_d) for _d in X_train[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
