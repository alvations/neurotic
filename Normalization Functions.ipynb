{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Functions\n",
    "\n",
    "Previously, we have covered how a single neuron would look like and when they \"activate\", they produce a \"squashing\" effect to force the output into some bounded domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistics\n",
    "\n",
    "Let's warm up with some simple calculations of mean, variance and standard deviation. \n",
    "\n",
    "\n",
    "## (Arithmetic) Mean\n",
    "The (arithmetic) mean of a list is the sum of its elements divided by the no. of elements:\n",
    "\n",
    "<img src=\"images/arithmetic-mean.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "m = x.sum() / len(x) # x_bar\n",
    "print 'Mean =', m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance \n",
    "\n",
    "The variance of a list of real numbers (historically aka as the \"population\" in statistics) is the measure of how spread out are the elements within the list.\n",
    "\n",
    "<img src=\"images/variance.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance = 1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "n = len(x)\n",
    "m = x.sum() / n\n",
    "s2 = sum((x - m)**2) / (n-1)\n",
    "print 'Variance =', s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation\n",
    "\n",
    "The standard deviation of a list of real numbers is simply the square-root of the variance, it quantifies the spread-out-ness of the elements in the list.\n",
    "\n",
    "<img src='images/standard-deviation.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 2.0\n",
      "Variance = 1.0\n",
      "Standard deviation = 1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "n = float(len(x))\n",
    "m = x.sum() / n\n",
    "s2 = sum((x - m)**2) / (n-1.) \n",
    "s = s2**0.5\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our statistics \"grabber\" into a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old statistics functions\n",
      "Mean = 2.0\n",
      "Variance = 1.0\n",
      "Standard deviation = 1.0\n",
      "\n",
      "With numpy\n",
      "Mean = 2.0\n",
      "Variance = 0.666666666667\n",
      "Standard deviation = 0.816496580928\n"
     ]
    }
   ],
   "source": [
    "def get_statistics_old(x):\n",
    "    n = float(len(x))\n",
    "    m = x.sum() / n\n",
    "    s2 = sum((x - m)**2) / (n-1.) \n",
    "    s = s2**0.5\n",
    "    return m, s2, s\n",
    "\n",
    "def get_statistics(x):\n",
    "    return np.mean(x), np.var(x), np.std(x)\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "m, s2, s = get_statistics_old(x)\n",
    "print 'Old statistics functions'\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s\n",
    "print \n",
    "m, s2, s = get_statistics(x)\n",
    "print 'With numpy'\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Compute the variance along the specified axis.\n",
      "\n",
      "    Returns the variance of the array elements, a measure of the spread of a\n",
      "    distribution.  The variance is computed for the flattened array by\n",
      "    default, otherwise over the specified axis.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing numbers whose variance is desired.  If `a` is not an\n",
      "        array, a conversion is attempted.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        Axis or axes along which the variance is computed.  The default is to\n",
      "        compute the variance of the flattened array.\n",
      "\n",
      "        .. versionadded: 1.7.0\n",
      "\n",
      "        If this is a tuple of ints, a variance is performed over multiple axes,\n",
      "        instead of a single axis or all the axes as before.\n",
      "    dtype : data-type, optional\n",
      "        Type to use in computing the variance.  For arrays of integer type\n",
      "        the default is `float32`; for arrays of float types it is the same as\n",
      "        the array type.\n",
      "    out : ndarray, optional\n",
      "        Alternate output array in which to place the result.  It must have\n",
      "        the same shape as the expected output, but the type is cast if\n",
      "        necessary.\n",
      "    ddof : int, optional\n",
      "        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n",
      "        ``N - ddof``, where ``N`` represents the number of elements. By\n",
      "        default `ddof` is zero.\n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the original `arr`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    variance : ndarray, see dtype parameter above\n",
      "        If ``out=None``, returns a new array containing the variance;\n",
      "        otherwise, a reference to the output array is returned.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    std , mean, nanmean, nanstd, nanvar\n",
      "    numpy.doc.ufuncs : Section \"Output arguments\"\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The variance is the average of the squared deviations from the mean,\n",
      "    i.e.,  ``var = mean(abs(x - x.mean())**2)``.\n",
      "\n",
      "    The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.\n",
      "    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n",
      "    instead.  In standard statistical practice, ``ddof=1`` provides an\n",
      "    unbiased estimator of the variance of a hypothetical infinite population.\n",
      "    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n",
      "    normally distributed variables.\n",
      "\n",
      "    Note that for complex numbers, the absolute value is taken before\n",
      "    squaring, so that the result is always real and nonnegative.\n",
      "\n",
      "    For floating-point input, the variance is computed using the same\n",
      "    precision the input has.  Depending on the input data, this can cause\n",
      "    the results to be inaccurate, especially for `float32` (see example\n",
      "    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n",
      "    keyword can alleviate this issue.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1, 2], [3, 4]])\n",
      "    >>> np.var(a)\n",
      "    1.25\n",
      "    >>> np.var(a, axis=0)\n",
      "    array([ 1.,  1.])\n",
      "    >>> np.var(a, axis=1)\n",
      "    array([ 0.25,  0.25])\n",
      "\n",
      "    In single precision, var() can be inaccurate:\n",
      "\n",
      "    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "    >>> a[0, :] = 1.0\n",
      "    >>> a[1, :] = 0.1\n",
      "    >>> np.var(a)\n",
      "    0.20250003\n",
      "\n",
      "    Computing the variance in float64 is more accurate:\n",
      "\n",
      "    >>> np.var(a, dtype=np.float64)\n",
      "    0.20249999932944759\n",
      "    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n",
      "    0.2025\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print np.var.func_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With numpy, default parameters\n",
      "Mean = 2.0\n",
      "Variance = 0.666666666667\n",
      "Standard deviation = 0.816496580928\n",
      "\n",
      "With numpy, degree of freedom=0\n",
      "Mean = 2.0\n",
      "Variance = 0.666666666667\n",
      "Standard deviation = 0.816496580928\n",
      "\n",
      "With numpy, degree of freedom=1\n",
      "Mean = 2.0\n",
      "Variance = 1.0\n",
      "Standard deviation = 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_statistics_ddf0(x):\n",
    "    return np.mean(x), np.var(x), np.std(x)\n",
    "\n",
    "def get_statistics_ddf1(x):\n",
    "    return np.mean(x), np.var(x, ddof=1), np.std(x, ddof=1)\n",
    "\n",
    "\n",
    "m, s2, s = get_statistics(x)\n",
    "print 'With numpy, default parameters'\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s\n",
    "print \n",
    "m, s2, s = get_statistics_ddf0(x)\n",
    "print 'With numpy, degree of freedom=0'\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s\n",
    "print \n",
    "m, s2, s = get_statistics_ddf1(x)\n",
    "print 'With numpy, degree of freedom=1'\n",
    "print 'Mean =', m\n",
    "print 'Variance =', s2\n",
    "print 'Standard deviation =', s\n",
    "\n",
    "get_statistics = get_statistics_ddf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember the Sigmoid?\n",
    "\n",
    "The sigmoid function also has similar \"abilities\" and in fact the softmax and the sigmoid functions are related. Previously, we saw how a single neuron would \"activate\" given an input and split a \"squashed\" output.\n",
    "\n",
    "The standard sigmoid activation function is:\n",
    "\n",
    "<img src=\"images/\">\n",
    "\n",
    "Now, we'll introduce the notion of normalized sigmoid where it's\n",
    "\n",
    "<img src=\"images/normalized-sigmoid.png\">\n",
    "\n",
    "We see that it substracts the `x_i` value by the mean and divide it by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def norm_sigmoid(x):\n",
    "    m = np.mean(x)\n",
    "    s = np.std(x)\n",
    "    return 1 / (1 + np.exp(-(x-m)/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[ 0.73105858  0.88079708  0.95257413]\n",
      "[ 0.22710252  0.5         0.77289748]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print x\n",
    "print sigmoid(x)\n",
    "print norm_sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard vs Normalized Sigmoid\n",
    "\n",
    "It seems like there's quite some difference in terms of the transformations that the standard and the normalized sigmoid functions are doing to the inputs.\n",
    "\n",
    "Let's look at it from another perspective where the inputs are highly skewed and contains outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0.5, 1, 10, 15, 20.3245, 500, 50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62245933,  0.73105858,  0.9999546 ,  0.99999969,  1.        ,\n",
       "        1.        ,  1.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x) # Generalize the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39809267,  0.39809953,  0.39822302,  0.39829162,  0.39836469,\n",
       "        0.40496512,  0.9205157 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sigmoid(x) # Generalize the \"inliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "A softmax function also squeezes the real number values into the 0 to 1 range; in addition, the special property of the function is that the elements in the list sums to 1. In fact, it's a generalized form of the logistic regression.\n",
    "\n",
    "<img src='images/softmax.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95656121  0.40232443  0.73815292]\n",
      "[ 0.42046703  0.24156275  0.33797022]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(3)\n",
    "print x\n",
    "print softmax(x)\n",
    "print softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09003057  0.24472847  0.66524096]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print softmax(x)\n",
    "print softmax(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relation between Softmax and Sigmoid\n",
    "\n",
    "Essentially, the softmax function is the generalized version of the sigmoid function. So if we try to differentiate the sigmoid function:\n",
    "\n",
    "<img src='images/differentiate-sigmoid.png'>\n",
    "\n",
    "Now, we love / like this function, especially when we need the derivative for optimization / back-propagation later ;P\n",
    "\n",
    "By this we have a relation between the 2nd and 1st order function (`y^2` and `y`) through the derivative (*Bernoulli differential equation*), so we the function `f(x)` has a solution:\n",
    "\n",
    "<img src='images/bernoulli-solution.png'>\n",
    "\n",
    "And in the case of sigmoid, it happens to be `C=1`.\n",
    "\n",
    "<img src='images/bernoulli-solution-sigmoid.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73105858  0.88079708  0.95257413] [ 0.09003057  0.24472847  0.66524096]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print sigmoid(x), softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid(x): [ 0.55230791  0.72908792  1.          1.          1.        ]\n",
      "NormalizeSigmoid(x): [ 0.34814876  0.3482413   0.87935657  0.40939807  0.40952447]\n",
      "Softmax(x): [  0.   0.  nan   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Do note that softmax isn't very good when it comes to highly skewed inputs, e.g.\n",
    "x = np.array([0.21, 0.99, 5000, 499, 500])\n",
    "print 'Sigmoid(x):', sigmoid(x)\n",
    "print 'NormalizeSigmoid(x):', norm_sigmoid(x)\n",
    "print 'Softmax(x):', softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to pass highly skewed input into a `softmax()` function such that you receive an output vector that sums to 1, you can try doing something like `softmax(sigmoid(x))` or `softmax(norm_sigmoid(x))`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14519143,  0.17326688,  0.22718056,  0.22718056,  0.22718056])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17159898,  0.17161486,  0.29188739,  0.18243785,  0.18246091])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(norm_sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the normalized sigmoid usually highlights the \"most prominent\" class. This is useful when it is placed before a final output layer when we want to do classification and requires 1 class to \"stand out\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL MATERIAL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relation between Softmax and Sigmoid\n",
    "\n",
    "Essentially, the softmax function is the generalized version of the sigmoid function. So if we try to differentiate the sigmoid function:\n",
    "\n",
    "<img src='images/differentiate-sigmoid.png'>\n",
    "\n",
    "Now, we love / like this function, especially when we need the derivative for optimization / back-propagation later ;P\n",
    "\n",
    "By this we have a relation between the 2nd and 1st order function (`y^2` and `y`) through the derivative (*Bernoulli differential equation*), so we the function `f(x)` has a solution:\n",
    "\n",
    "<img src='images/bernoulli-solution.png'>\n",
    "\n",
    "And in the case of sigmoid, it happens to be `C=1`.\n",
    "\n",
    "<img src='images/bernoulli-solution-sigmoid.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tanh Function\n",
    "\n",
    "The `tanh()` activation function also a kind of softmax that looks very similar to the `sigmoid()`.\n",
    "Imagine you double the \"steepness\" of the slope of the sigmoid curve and then move the\n",
    "\n",
    "(I would like to think it's named after me, but it isn't ;P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
